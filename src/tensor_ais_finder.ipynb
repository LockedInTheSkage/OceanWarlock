{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press any key to continue . . . \n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "%pip install pandas \n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from feature_engineering.tensor_features import develop_features, floating_conv\n",
    "from path_finder import path_sorter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from data_handler import LocalToLargeDataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving training data...\n"
     ]
    }
   ],
   "source": [
    "data_loader = LocalToLargeDataLoader(print_progress=True)\n",
    "parsed_data = data_loader.load_raw_data(path=\"../../resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['time', 'cog', 'sog', 'rot', 'heading', 'navstat', 'latitude',\n",
      "       'longitude', 'vesselId', 'portId', 'etaParsed', 'UN_LOCODE', 'ISO',\n",
      "       'portLongitude', 'portLatitude'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(parsed_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===================-] 100.0% complete\n",
      "[====================] 100.0% complete\n",
      "Concatting dataframes\n",
      "Number of dataframes: 152370\n"
     ]
    }
   ],
   "source": [
    "path_dict = path_sorter(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting into tests and training data\n",
      "Developing features with the following columns:  Index(['rot', 'heading', 'navstat', 'etaParsed', 'UN_LOCODE', 'ISO',\n",
      "       'portLongitude', 'portLatitude', 'time_0', 'time_1', 'cog_1', 'sog_1',\n",
      "       'latitude_1', 'longitude_1', 'time_2', 'cog_2', 'sog_2', 'latitude_2',\n",
      "       'longitude_2', 'time_3', 'cog_3', 'sog_3', 'latitude_3', 'longitude_3',\n",
      "       'time_4', 'cog_4', 'sog_4', 'latitude_4', 'longitude_4', 'time_5',\n",
      "       'cog_5', 'sog_5', 'latitude_5', 'longitude_5', 'time_6', 'cog_6',\n",
      "       'sog_6', 'latitude_6', 'longitude_6', 'time_7', 'cog_7', 'sog_7',\n",
      "       'latitude_7', 'longitude_7', 'time_8', 'cog_8', 'sog_8', 'latitude_8',\n",
      "       'longitude_8', 'time_9', 'cog_9', 'sog_9', 'latitude_9', 'longitude_9'],\n",
      "      dtype='object')\n",
      "Categorizing...\n",
      "Tokenizing...\n",
      "Normalizing timestamps...\n",
      "Derivating locations...\n",
      "Developing features with the following columns:  Index(['rot', 'heading', 'navstat', 'etaParsed', 'UN_LOCODE', 'ISO',\n",
      "       'portLongitude', 'portLatitude', 'time_0', 'time_1', 'cog_1', 'sog_1',\n",
      "       'latitude_1', 'longitude_1', 'time_2', 'cog_2', 'sog_2', 'latitude_2',\n",
      "       'longitude_2', 'time_3', 'cog_3', 'sog_3', 'latitude_3', 'longitude_3',\n",
      "       'time_4', 'cog_4', 'sog_4', 'latitude_4', 'longitude_4', 'time_5',\n",
      "       'cog_5', 'sog_5', 'latitude_5', 'longitude_5', 'time_6', 'cog_6',\n",
      "       'sog_6', 'latitude_6', 'longitude_6', 'time_7', 'cog_7', 'sog_7',\n",
      "       'latitude_7', 'longitude_7', 'time_8', 'cog_8', 'sog_8', 'latitude_8',\n",
      "       'longitude_8', 'time_9', 'cog_9', 'sog_9', 'latitude_9', 'longitude_9'],\n",
      "      dtype='object')\n",
      "Categorizing...\n",
      "Tokenizing...\n",
      "Normalizing timestamps...\n",
      "Derivating locations...\n"
     ]
    }
   ],
   "source": [
    "features, y, test_features, y_test = data_loader.load_training_data(path_dict)\n",
    "\n",
    "features=develop_features(features, derivative_locations=True)\n",
    "test_features=develop_features(test_features, derivative_locations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rot', 'heading', 'navstat', 'etaParsed', 'UN_LOCODE', 'ISO', 'portLongitude', 'portLatitude', 'time_0', 'time_1', 'cog_1', 'sog_1', 'latitude_1', 'longitude_1', 'time_2', 'cog_2', 'sog_2', 'latitude_2', 'longitude_2', 'time_3', 'cog_3', 'sog_3', 'latitude_3', 'longitude_3', 'time_4', 'cog_4', 'sog_4', 'latitude_4', 'longitude_4', 'time_5', 'cog_5', 'sog_5', 'latitude_5', 'longitude_5', 'time_6', 'cog_6', 'sog_6', 'latitude_6', 'longitude_6', 'time_7', 'cog_7', 'sog_7', 'latitude_7', 'longitude_7', 'time_8', 'cog_8', 'sog_8', 'latitude_8', 'longitude_8', 'time_9', 'cog_9', 'sog_9', 'latitude_9', 'longitude_9']\n",
      "['rot', 'heading', 'navstat', 'etaParsed', 'UN_LOCODE', 'ISO', 'portLongitude', 'portLatitude', 'time_0', 'time_1', 'cog_1', 'sog_1', 'latitude_1', 'longitude_1', 'time_2', 'cog_2', 'sog_2', 'latitude_2', 'longitude_2', 'time_3', 'cog_3', 'sog_3', 'latitude_3', 'longitude_3', 'time_4', 'cog_4', 'sog_4', 'latitude_4', 'longitude_4', 'time_5', 'cog_5', 'sog_5', 'latitude_5', 'longitude_5', 'time_6', 'cog_6', 'sog_6', 'latitude_6', 'longitude_6', 'time_7', 'cog_7', 'sog_7', 'latitude_7', 'longitude_7', 'time_8', 'cog_8', 'sog_8', 'latitude_8', 'longitude_8', 'time_9', 'cog_9', 'sog_9', 'latitude_9', 'longitude_9']\n",
      "54\n",
      "        latitude_0  longitude_0\n",
      "39290     35.45642    139.68042\n",
      "46876     34.47404    138.54770\n",
      "6999     -26.74072    153.65371\n",
      "108418    39.24151    -76.52550\n",
      "51143     51.31398      3.22757\n"
     ]
    }
   ],
   "source": [
    "print(list(features.keys()))\n",
    "print(list(test_features.keys()))\n",
    "print(len(list(test_features.keys())))\n",
    "missing_columns=[col for col in list(features.keys()) if not col in list(test_features.keys())]\n",
    "if missing_columns:\n",
    "    test_features[missing_columns]=0\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=features.astype(float)\n",
    "test_features=test_features.astype(float)\n",
    "\n",
    "X_train = features.to_numpy()\n",
    "X_test_np =test_features.to_numpy()\n",
    "y_train = y.to_numpy()\n",
    "y_test_np = y_test.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  3.00000000e+02  5.00000000e+00 ...  0.00000000e+00\n",
      "  -0.00000000e+00 -7.92393027e-04]\n",
      " [-3.00000000e+00  1.14000000e+02  0.00000000e+00 ...  1.36000000e+01\n",
      "   5.33415334e+00  8.52808528e-01]\n",
      " [ 0.00000000e+00  1.86000000e+02  0.00000000e+00 ...  9.00000000e-01\n",
      "  -2.74365274e-01 -3.51351351e-01]\n",
      " ...\n",
      " [ 0.00000000e+00  3.33000000e+02  5.00000000e+00 ...  0.00000000e+00\n",
      "   9.26784060e-04 -0.00000000e+00]\n",
      " [ 0.00000000e+00  4.40000000e+01  0.00000000e+00 ...  1.77000000e+01\n",
      "   5.65873016e+00  9.44742063e+00]\n",
      " [ 0.00000000e+00  1.73000000e+02  5.00000000e+00 ...  0.00000000e+00\n",
      "  -0.00000000e+00  3.17712470e-03]]\n",
      "54\n",
      "        latitude_0  longitude_0\n",
      "39290     35.45642    139.68042\n",
      "46876     34.47404    138.54770\n",
      "6999     -26.74072    153.65371\n",
      "108418    39.24151    -76.52550\n",
      "51143     51.31398      3.22757\n",
      "...            ...          ...\n",
      "119209    55.53064     12.75990\n",
      "61294     53.56551      8.56709\n",
      "60261     51.30153      3.23734\n",
      "54272     52.64461      3.97908\n",
      "130678    43.58090     10.30401\n",
      "\n",
      "[121896 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(X_test_np.shape[1])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bruker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "input_dim = X_train.shape[1]  # Number of features in your dataset\n",
    "\n",
    "# Add layers\n",
    "model.add(Dense(64, input_dim=input_dim, activation='relu'))  # First hidden layer\n",
    "\n",
    "layers=1\n",
    "dropout=0.1\n",
    "nodes=input_dim*2\n",
    "\n",
    "\n",
    "for layer in range(layers):\n",
    "    model.add(Dense(nodes, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "# If it's a regression task (predicting a continuous variable like time):\n",
    "model.add(Dense(units=2, activation='linear')) \n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m3048/3048\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 8ms/step - loss: 2943.2151 - mae: 33.6455 - val_loss: 2614.4143 - val_mae: 30.7477\n",
      "Epoch 2/100\n",
      "\u001b[1m1665/3048\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 2645.2388 - mae: 31.5402"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, \n",
    "                    validation_split=0.2, callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m953/953\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2622.5747 - mae: 30.9863\n",
      "Test Loss: 2610.961669921875, Test MAE: 30.82440757751465\n"
     ]
    }
   ],
   "source": [
    "loss, mae = model.evaluate(X_test_np, y_test)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m953/953\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "[[36.683044 11.225968]\n",
      " [36.683044 11.225968]\n",
      " [36.683044 11.225968]\n",
      " ...\n",
      " [36.683044 11.225968]\n",
      " [36.683044 11.225968]\n",
      " [36.683044 11.225968]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_np) \n",
    "\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
