{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance, plot_tree\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "\n",
    "import loadBar\n",
    "from csv_parser import CSVParser\n",
    "from globals import RESOURCE_FOLDER, STEPSIZES, OUTPUT_WINDOW, INPUT_WINDOW, OUTPUT_FORECAST, DELETEABLE_COLUMNS, ONE_HOT_COLUMNS\n",
    "from markovSquares import apply_markov\n",
    "from feature_engineer import FeatureEngineer\n",
    "from exploring_data_functions import *\n",
    "\n",
    "from searoutePointFinder import fill_with_proximity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CSVParser(RESOURCE_FOLDER)\n",
    "index_data = parser.retrieve_training_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_data.set_index(\"time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampler(df, sorting_column, freq):\n",
    "    unique_ids = df[sorting_column].unique()\n",
    "    final_df = pd.DataFrame()\n",
    "    partial_list = []\n",
    "\n",
    "    for i in range(len(unique_ids)):\n",
    "        loadBar.load_bar(len(unique_ids),i+1)\n",
    "        resample_partial = df[df[sorting_column] == unique_ids[i]].resample(freq).last()\n",
    "\n",
    "        resample_partial = fill_with_proximity(resample_partial)\n",
    "        partial_list.append(resample_partial)\n",
    "\n",
    "    for chunk in partial_list:\n",
    "        final_df = pd.concat([final_df,chunk])\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "print(index_data)\n",
    "resampled_data_20min = resampler(index_data, \"vesselId\", \"20min\")\n",
    "\n",
    "resampled_data_20min.to_csv(\"../../Project materials(1)/data_resampled_20min.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.read_csv(RESOURCE_FOLDER+\"/data_resampled_20min.csv\")\n",
    "total_df[\"time\"] = pd.to_datetime(total_df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering_functions = [categorize_navstat, numerize_UN_LOCODE, numerize_ISO, apply_markov, type_dummies] #, daystoETAparsed, categorize_rot\n",
    "\n",
    "\n",
    "feature_engineer = FeatureEngineer(total_df)\n",
    "feature_engineer.apply_features(feature_engineering_functions)\n",
    "total_df = feature_engineer.get_dataframe()\n",
    "\n",
    "total_df.set_index(\"time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "one_hot_columns = ONE_HOT_COLUMNS\n",
    "total_df = pd.get_dummies(total_df, columns=one_hot_columns, drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make time series into supervised problem\n",
    "\n",
    "# 1 = 20 minutes\n",
    "# 3 = 1 hour\n",
    "# 18 = 6 hours\n",
    "# 72 = 24 hours\n",
    "# 144 = 2 days\n",
    "# 216 = 3 days\n",
    "# 288 = 4 days\n",
    "# 360 = 5 days\n",
    "\n",
    "\n",
    "\n",
    "def make_supervised(df, forecast_columns, sorting_column, input_window=1, output_window=1, stepsize=1):\n",
    "    \"\"\"\n",
    "    Converts a multivariate time series dataframe into a supervised learning problem.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The original dataframe with time series data.\n",
    "    forecast_columns (list): A list of column names to forecast.\n",
    "    input_window (int): The number of past observations to use as features.\n",
    "    output_window (int): The number of steps to forecast into the future.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A new dataframe with supervised learning format.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    df_new = pd.DataFrame()\n",
    "    #Put in a for loop here where you iterate over all IDs, to make sure things get correct\n",
    "    unique_sorts = df[sorting_column].unique()\n",
    "\n",
    "    #Iterate through all IDs\n",
    "    print(\"Creating supervised data\")\n",
    "    for i, sorts in enumerate(unique_sorts):\n",
    "        loadBar.load_bar(len(unique_sorts),i+1)\n",
    "        sort_df = df[df[sorting_column] == sorts]\n",
    "        for start in range(stepsize):\n",
    "            sort_df = sort_df.iloc[start::stepsize]\n",
    "        \n",
    "            df_supervised = supervise_single_vessel_df(sort_df, forecast_columns, input_window, output_window)\n",
    "        \n",
    "            df_new = pd.concat([df_new, df_supervised])\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "\n",
    "def supervise_single_vessel_df(sort_df, forecast_columns, input_window=1, output_window=1):\n",
    "    df_supervised = pd.DataFrame()\n",
    "    other_cols = [col for col in sort_df.columns if col not in forecast_columns]\n",
    "\n",
    "    #Iterate through all columns for input features\n",
    "    for col in forecast_columns: \n",
    "        for i in range(input_window, 0, -1):\n",
    "            df_supervised[f\"{col}_t-{i}\"] = sort_df[col].shift(i)\n",
    "        \n",
    "        df_supervised[f\"{col}_t\"] = sort_df[col]\n",
    "        \n",
    "\n",
    "    # Create columns for forecast (target) with forward shift\n",
    "    for col in forecast_columns:\n",
    "        for j in range(output_window, 0,-1):\n",
    "            df_supervised[f\"{col}_t+{j}\"] = sort_df[col].shift(-j)\n",
    "\n",
    "    df_supervised = df_supervised.dropna()\n",
    "\n",
    "    df_supervised[other_cols] = sort_df[other_cols]\n",
    "\n",
    "    return df_supervised\n",
    "\n",
    "\n",
    "def supervise_on_stepsize(df, stepsize, output_forecast, sorting_column, input_window, output_window):\n",
    "    \n",
    "\n",
    "    forecast_columns = [\"lat\", \"lon\"]\n",
    "    supervised_df = make_supervised(df, forecast_columns, sorting_column, input_window, output_window)\n",
    "    supervised_df.to_csv(f\"../../build_resources/supervised_{stepsize}_input_{input_window}_output_{output_window}.csv\")\n",
    "    return supervised_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = make_supervised(total_df, OUTPUT_FORECAST, \"vesselId\", input_window=INPUT_WINDOW, output_window=OUTPUT_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = total_df.drop(DELETEABLE_COLUMNS, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Sorting columns\n",
    "def sort_columns(df):\n",
    "    selected_columns = df.filter(regex=r'_t$|_t\\+|_t-')\n",
    "    non_selected_columns = df.drop(selected_columns.columns, axis=1)\n",
    "    # Extract suffixes and assign _t as _t0\n",
    "    columns_with_suffix = []\n",
    "    for col in selected_columns.columns:\n",
    "        match = re.search(r\"_t([+-]?\\d*)$\", col)\n",
    "        # If there's no number after _t, treat it as _t0\n",
    "        suffix = int(match.group(1)) if match.group(1) else 0\n",
    "        columns_with_suffix.append((col, suffix))\n",
    "    \n",
    "    # Sort by suffix value (ascending)\n",
    "    sorted_t_columns = [col for col, _ in sorted(columns_with_suffix, key=lambda x: x[1])]\n",
    "    \n",
    "    # Reorder dataframe columns\n",
    "    return df[sorted_t_columns+non_selected_columns.columns.tolist()]\n",
    "\n",
    "total_df = total_df.dropna()\n",
    "print(len(total_df))\n",
    "total_df = total_df.sort_index(ascending = True)\n",
    "total_df=sort_columns(total_df)\n",
    "\n",
    "print(total_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_df.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_test_split(df, perc1, perc2, output_window):\n",
    "    y_list = []\n",
    "    for j in range(output_window):\n",
    "        for col in OUTPUT_FORECAST:\n",
    "            y_list.append(f\"{col}_t+{j+1}\")\n",
    "    ys = df[y_list]\n",
    "    Xs = df.drop(columns = y_list)\n",
    "\n",
    "    X_train = Xs.iloc[:int(np.round(Xs.shape[0]*perc1)),:]\n",
    "    y_train = ys.iloc[:int(np.round(Xs.shape[0]*perc1)),:]\n",
    "    X_val = Xs.iloc[int(np.round(Xs.shape[0]*perc1)):int(np.round(Xs.shape[0]*perc2)),:]\n",
    "    y_val = ys.iloc[int(np.round(Xs.shape[0]*perc1)):int(np.round(Xs.shape[0]*perc2)),:]\n",
    "    X_test = Xs.iloc[int(np.round(Xs.shape[0]*perc2)):,:]\n",
    "    y_test = ys.iloc[int(np.round(Xs.shape[0]*perc2)):,:]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_test_split(total_df, 0.85, 0.99, OUTPUT_WINDOW)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(stepsize, preds, y_val):\n",
    "    print(\"/\"+\"-\"*50+\"\\\\\")\n",
    "    print(\"Evaluating model with stepsize\", stepsize)\n",
    "\n",
    "    results = {\n",
    "        \"MAE\": mean_absolute_error(y_val, preds),\n",
    "        \"MSE\": np.square(np.subtract(y_val,preds)).mean(),\n",
    "        \"R2 Score\": r2_score(y_val, preds),\n",
    "        \"RMSE\": np.sqrt(np.square(np.subtract(y_val,preds)).mean())\n",
    "    }\n",
    "\n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(\"\\\\\"+\"-\"*50+\"/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Tuning params\n",
    "# # We need to use XGB\n",
    "\n",
    "# # Define model with a high num_boost_round\n",
    "# model = xgb.XGBRFRegressor(\n",
    "#     objective=\"reg:squarederror\",\n",
    "#     tree_method=\"hist\",  # or \"hist\" if not using GPU\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# # Define parameter grid\n",
    "# param_grid = {\n",
    "#     \"max_depth\": [3, 5, 7],\n",
    "#     \"learning_rate\": [1],\n",
    "#     \"subsample\": [0.6, 0.8, 1.0],\n",
    "#     \"colsample_bynode\": [0.4, 0.6, 0.8],\n",
    "#     \"num_parallel_tree\": [50, 100, 200]\n",
    "# }\n",
    "\n",
    "# # Use TimeSeriesSplit for time series cross-validation\n",
    "# tscv = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "# # Set early stopping and validation set in fit parameters\n",
    "# fit_params = {\n",
    "#     \"eval_set\": [(X_val, y_val)],  # Validation set to monitor performance\n",
    "#     \"verbose\": 1\n",
    "# }\n",
    "\n",
    "# # RandomizedSearchCV with early stopping\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=model,\n",
    "#     param_distributions=param_grid,\n",
    "#     n_iter=20,  # Number of sampled parameter combinations\n",
    "#     scoring=\"neg_mean_squared_error\",\n",
    "#     cv=tscv,\n",
    "#     verbose=1,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Fit with early stopping\n",
    "# random_search.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "# # Output best parameters and number of boosting rounds\n",
    "# best_params = random_search.best_params_\n",
    "# best_num_boost_round = model.get_booster().best_iteration  # Retrieve best boosting rounds\n",
    "# print(\"Best parameters:\", best_params)\n",
    "# print(\"Best num_boost_round:\", best_num_boost_round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting_columns=X_train.columns.to_list()\n",
    "fitting_types=X_train.dtypes.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(X_train)\n",
    "\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "dtest_X = xgb.DMatrix(X_test)\n",
    "\n",
    "params = {\"objective\": \"reg:squarederror\",\n",
    "            \"max_depth\": 7,\n",
    "            \"booster\": \"gbtree\",\n",
    "            \"tree-method\": \"gpu_hist\",\n",
    "            \"colsample_bynode\": 0.4,\n",
    "            \"num_parallel_tree\": 50,\n",
    "            \"subsample\": 0.6,\n",
    "            \"seed\": 42,\n",
    "            \"learning_rate\": 1,\n",
    "            #\"n_estimators\": 100,\n",
    "            #\"reg_alpha\": 0.1,\n",
    "            #\"reg_lambda\": 0.1,\n",
    "            #\"n_jobs\": -1,\n",
    "            \"verbosity\": 1\n",
    "            }\n",
    "\n",
    "#Subsample was 0.8, num-parallel tree was 100, colsample-bynode was 0.5 and max-depth was 5. Before tuning. \n",
    "\n",
    "#After first round of tuning we got sumbsample was 0.6, num-parallel tree was 50, colsample-bynode was 0.4 and max-depth was 7. Before tuning. \n",
    "\n",
    "num_boost_round = 30\n",
    "\n",
    "#Tried 50, but that stagnated quickly. So reducing again to 30.  \n",
    "\n",
    "\n",
    "early_stopping_rounds = 3\n",
    "\n",
    "print(dtrain)\n",
    "\n",
    "model = xgb.train(params, dtrain, num_boost_round, evals=[(dval, \"validation\")], \n",
    "                  early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
    "\n",
    "\n",
    "preds = model.predict(dtest_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_n_min_mark(timestamp, n=1):\n",
    "    timestamp = pd.to_datetime(timestamp)\n",
    "    minutes = timestamp.minute\n",
    "    closest_mark = round(minutes / (n*20)) * n*20\n",
    "    if closest_mark == 60:\n",
    "        rounded_timestamp = timestamp.replace(minute=0, second=0, microsecond=0) + pd.Timedelta(hours=1)\n",
    "    else:\n",
    "        rounded_timestamp = timestamp.replace(minute=closest_mark, second=0, microsecond=0)\n",
    "    \n",
    "    return rounded_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_to_back(process_df):      \n",
    "    for _, col in enumerate(OUTPUT_FORECAST):\n",
    "\n",
    "        max_suffix_neg = 0\n",
    "        max_suffix_pos = 0\n",
    "        \n",
    "        # Identify existing suffixes in the process_df for the current column\n",
    "        while f\"{col}_t-{max_suffix_neg+1}\" in process_df.columns:\n",
    "            max_suffix_neg += 1\n",
    "        while f\"{col}_t+{max_suffix_pos+1}\" in process_df.columns:\n",
    "            max_suffix_pos += 1\n",
    "        for shift in range(max_suffix_neg - 1, -max_suffix_pos, -1):  # Start from max_suffix-1 down to 0\n",
    "            if shift == 0:\n",
    "                # Set the new predicted value as the most recent\n",
    "                process_df[f\"{col}_t\"] = process_df[f\"{col}_t+1\"]\n",
    "            elif shift == 1:\n",
    "                # Shift the column\n",
    "                process_df[f\"{col}_t-{shift}\"] = process_df[f\"{col}_t\"]\n",
    "            elif shift > 1:\n",
    "                # Shift the column\n",
    "                process_df[f\"{col}_t-{shift}\"] = process_df[f\"{col}_t-{shift - 1}\"]\n",
    "            else:\n",
    "                process_df[f\"{col}_t+{-shift}\"] = process_df[f\"{col}_t+{-shift + 1}\"]\n",
    "\n",
    "        for shift in range(1, max_suffix_pos+1):\n",
    "            process_df = process_df.drop(columns=[f\"{col}_t+{shift}\"])\n",
    "    \n",
    "    return process_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_features=[apply_markov]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_far_future(model, features, test_df,  forecast_columns):\n",
    "    \n",
    "    X_test = features.copy().iloc[-1:]\n",
    "    preds = pd.DataFrame(columns=[\"vesselId\", \"approximate_time\"])\n",
    "    \n",
    "    # Determine the furthest time in 20-minute intervals\n",
    "    furthest_time = closest_n_min_mark(test_df[\"time\"].max())\n",
    "    current_time = closest_n_min_mark(X_test.index.max())\n",
    "    \n",
    "    # Generate the future time steps at 20-minute intervals\n",
    "    future_steps = pd.date_range(start=current_time, end=furthest_time, freq='20min')\n",
    "    \n",
    "    for future_time in future_steps:\n",
    "        y_pred = model.predict(xgb.DMatrix(X_test))\n",
    "\n",
    "        new_row = pd.DataFrame({\n",
    "            \"vesselId\": [test_df[\"vesselId\"].iloc[0]],\n",
    "            \"approximate_time\": [future_time]\n",
    "        })\n",
    "        for idx, col in enumerate(forecast_columns):\n",
    "            new_row[f\"{col}\"] = y_pred[0, idx]  # Use the predicted value\n",
    "        \n",
    "        updateable_feature_engineer = FeatureEngineer(new_row)\n",
    "        updateable_feature_engineer.apply_features(update_features)\n",
    "        new_row = updateable_feature_engineer.get_dataframe()\n",
    "\n",
    "        selected_columns = X_test.filter(regex=r'_t$|_t\\+|_t-')\n",
    "        non_selected_columns = X_test.drop(selected_columns.columns, axis=1)\n",
    "        new_row[non_selected_columns.columns] = X_test[non_selected_columns.columns].iloc[0]\n",
    "        \n",
    "        \n",
    "        preds = pd.concat([preds, new_row], ignore_index=True)\n",
    "        \n",
    "        # Update X_test for the next iteration\n",
    "        for idx, col in enumerate(forecast_columns):\n",
    "\n",
    "            max_suffix = 0\n",
    "            \n",
    "            # Identify existing suffixes in the X_test for the current column\n",
    "            while f\"{col}_t-{max_suffix+1}\" in X_test.columns:\n",
    "                max_suffix += 1\n",
    "            for shift in range(max_suffix - 1, -1, -1):  # Start from max_suffix-1 down to 0\n",
    "                if shift == 0:\n",
    "                    # Set the new predicted value as the most recent\n",
    "                    X_test[f\"{col}_t\"] = y_pred[0, idx]\n",
    "                elif shift == 1:\n",
    "                    # Shift the column\n",
    "                    X_test[f\"{col}_t-{shift}\"] = X_test[f\"{col}_t\"]\n",
    "                else:\n",
    "                    # Shift the column\n",
    "                    X_test[f\"{col}_t-{shift}\"] = X_test[f\"{col}_t-{shift - 1}\"]\n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n",
    "csv_parser = CSVParser(folderpath=\"../../Project materials(1)\")\n",
    "\n",
    "test_df = csv_parser.retrieve_test_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_columns(b, columns, dtypes):\n",
    "    \"\"\"\n",
    "    Add missing columns to DataFrame b based on provided column names and data types.\n",
    "\n",
    "    Parameters:\n",
    "        b (pd.DataFrame): The target DataFrame to modify.\n",
    "        columns (list): List of column names that should be present.\n",
    "        dtypes (dict): Dictionary of column names and their desired data types.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The modified DataFrame b with missing columns added.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col not in b.columns:\n",
    "            # Determine the default value based on the type\n",
    "            if pd.api.types.is_numeric_dtype(dtypes[col]):\n",
    "                b[col] = 0  # Fill numeric columns with 0\n",
    "            elif pd.api.types.is_bool_dtype(dtypes[col]):\n",
    "                b[col] = False  # Fill boolean columns with False\n",
    "            elif pd.api.types.is_datetime64_any_dtype(dtypes[col]):\n",
    "                b[col] = pd.NaT  # Fill datetime columns with NaT (Not a Time)\n",
    "            else:\n",
    "                b[col] = 0  # Default for any other type, can customize as needed\n",
    "            b[col] = b[col].astype(dtypes[col])\n",
    "    \n",
    "    b=b[columns]\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(latest_features, feature_engineering_functions):\n",
    "\n",
    "    feature_engineer = FeatureEngineer(latest_features)\n",
    "    feature_engineer.apply_features(feature_engineering_functions)\n",
    "    latest_features = feature_engineer.get_dataframe()\n",
    "    latest_features = pd.get_dummies(latest_features, columns=ONE_HOT_COLUMNS, drop_first=False)\n",
    "    \n",
    "    latest_features = make_supervised(latest_features, OUTPUT_FORECAST, \"vesselId\" , INPUT_WINDOW, OUTPUT_WINDOW)\n",
    "    latest_features = shift_to_back(latest_features)\n",
    "    latest_features = latest_features.dropna()\n",
    "    latest_features = latest_features.drop(DELETEABLE_COLUMNS, axis=1)\n",
    "    latest_features = sort_columns(latest_features)\n",
    "\n",
    "    \n",
    "    latest_features=add_missing_columns(latest_features, fitting_columns, fitting_types)\n",
    "    \n",
    "    return latest_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_times(model,total_df,test_df):\n",
    "    unique_sorts = test_df[\"vesselId\"].unique()\n",
    "    preds_df = pd.DataFrame()\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    for sorts in unique_sorts:\n",
    "        latest_features=total_df[total_df[\"vesselId\"] == sorts]\n",
    "        test_by_vessel_df = test_df[test_df[\"vesselId\"] == sorts]\n",
    "\n",
    "        latest_features = preprocess(latest_features, feature_engineering_functions)\n",
    "\n",
    "        preds = predict_far_future(model, latest_features, test_by_vessel_df, OUTPUT_FORECAST)\n",
    "        preds_df = pd.concat([preds_df, preds])\n",
    "    \n",
    "    for test in test_df.iterrows():\n",
    "        test=pd.Series(test[1])\n",
    "        new_row = pd.DataFrame()\n",
    "        new_row=preds_df[\n",
    "            (preds_df[\"vesselId\"] == test[\"vesselId\"]) & \n",
    "            (preds_df[\"approximate_time\"] == closest_n_min_mark(test[\"time\"]))\n",
    "            ][[\"latitude\", \"longitude\"]]\n",
    "        new_row[\"ID\"] = test[\"ID\"]\n",
    "        new_row[\"time\"] = test[\"time\"]\n",
    "        \n",
    "        result = pd.concat([result, new_row])\n",
    "    result[\"latitude_predicted\"] = result[\"latitude\"]\n",
    "    result[\"longitude_predicted\"] = result[\"longitude\"]\n",
    "\n",
    "    return result[[\"ID\",\"longitude_predicted\",\"latitude_predicted\"]]\n",
    "\n",
    "print(test_df)\n",
    "total_df = pd.read_csv(\"../../Project materials(1)/data_resampled_20min.csv\")\n",
    "total_df[\"time\"] = pd.to_datetime(total_df['time'])\n",
    "total_df.set_index(\"time\", inplace=True)\n",
    "print(total_df.head())\n",
    "result_df = predict_times(model, total_df, test_df)\n",
    "print(result_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn results into a csv file\n",
    "result_df.to_csv(\"../../Project materials(1)/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(model, height=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model:\n",
    "Included navstat and etaParsed\n",
    "\n",
    "Timewindow: (3,2)\n",
    "\n",
    "MAE: 0.8521843262281953 \n",
    "\n",
    "MSE: longitude_t+1    21.225563\n",
    "\n",
    "latitude_t+1      1.993130\n",
    "\n",
    "longitude_t+2    38.471488\n",
    "\n",
    "latitude_t+2      3.840146\n",
    "\n",
    "dtype: float64\n",
    "\n",
    "R2 Score: 0.9958523607729776\n",
    "\n",
    "RMSE: longitude_t+1    4.607121\n",
    "\n",
    "latitude_t+1     1.411783\n",
    "\n",
    "longitude_t+2    6.202539\n",
    "\n",
    "latitude_t+2     1.959629\n",
    "\n",
    "dtype: float64\n",
    "\n",
    "\n",
    "### Second model:\n",
    "\n",
    "Added cog, rot and heading to target features.\n",
    "\n",
    "Timewindow: (3,2)\n",
    "\n",
    "MAE: 7.198335594601071\n",
    "MSE: latitude_t+1        1.980426\n",
    "longitude_t+1      21.577318\n",
    "cog_t+1          1820.208937\n",
    "rot_t+1            92.532501\n",
    "heading_t+1      1172.604934\n",
    "latitude_t+2        3.813640\n",
    "longitude_t+2      39.218475\n",
    "cog_t+2          2370.325440\n",
    "rot_t+2           107.991661\n",
    "heading_t+2      1769.347459\n",
    "dtype: float64\n",
    "R2 Score: 0.8826565909012996\n",
    "RMSE: latitude_t+1      1.407276\n",
    "longitude_t+1     4.645139\n",
    "cog_t+1          42.663907\n",
    "rot_t+1           9.619382\n",
    "heading_t+1      34.243320\n",
    "latitude_t+2      1.952854\n",
    "longitude_t+2     6.262466\n",
    "cog_t+2          48.685988\n",
    "rot_t+2          10.391904\n",
    "heading_t+2      42.063612\n",
    "dtype: float64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model:\n",
    "\n",
    "Added Navstat again, but this time categorical. Also made rot categorical. This time we did not predict future values of other things than long and lat. Think we will predict future values of other things again.\n",
    "\n",
    "Timewindow: (4,1)\n",
    "\n",
    "Kaggle: 174.1591\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth Model\n",
    "\n",
    "Got rid of rot_cat, and put in tuned hyperparameters. We think It may be confused about time window, because the model interpretation does not make sense. We will will reduce the time window for the next model.\n",
    "\n",
    "Timewindow: (8,1)\n",
    "\n",
    "Kaggle: 243.6678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Fourth Model\n",
    "\n",
    " Changed timewindow to 2,1., and put in tuned hyperparameters. Chose to not use 50 boosting rounds.\n",
    "\n",
    " Kaggle 177"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Fifth Model\n",
    "\n",
    " Changed timewindow to 1,1. Used 30 boosting rounds.\n",
    "\n",
    " Kaggle 190"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Sixth Model\n",
    "\n",
    " Changed timewind t 4,1 Used 30 boosting rounds\n",
    "\n",
    " Kaggle 171"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
