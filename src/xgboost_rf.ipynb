{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance, plot_tree\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "import loadBar\n",
    "from csv_parser import CSVParser\n",
    "from globals import RESOURCE_FOLDER\n",
    "from markovSquares import MarkovSquares\n",
    "from data_handler import LocalToLargeDataLoader\n",
    "from searoutePointFinder import fill_with_proximity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = LocalToLargeDataLoader(print_progress=True)\n",
    "parsed_data = data_loader.load_raw_data(path=\"../../resources\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_data = parsed_data.copy()\n",
    "index_data.set_index(\"time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampler(df, sorting_column, freq):\n",
    "    unique_ids = df[sorting_column].unique()\n",
    "    final_df = pd.DataFrame()\n",
    "    partial_list = []\n",
    "\n",
    "    for i in range(len(unique_ids)):\n",
    "        loadBar.load_bar(len(unique_ids),i+1)\n",
    "        resample_partial = df[df[sorting_column] == unique_ids[i]].resample(freq).last()\n",
    "\n",
    "        resample_partial = fill_with_proximity(resample_partial)\n",
    "        partial_list.append(resample_partial)\n",
    "\n",
    "    for chunk in partial_list:\n",
    "        final_df = pd.concat([final_df,chunk])\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "print(index_data)\n",
    "resampled_data_20min = resampler(index_data, \"vesselId\", \"20min\")\n",
    "\n",
    "resampled_data_20min.to_csv(\"../../resources/data_resampled_20min.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resampled_data_20min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovSquares():\n",
    "\n",
    "    def __init__(self, square_size):\n",
    "        self.square_size = square_size\n",
    "        self.normalized_markov_matrix = None\n",
    "        self.direction_columns = [\"SW\", \"S\", \"SE\", \"W\", \"C\", \"E\", \"NW\", \"N\", \"NE\"]\n",
    "\n",
    "    def markovSquares(self, df, sorting_column):\n",
    "        unique_sorts = df[sorting_column].unique()\n",
    "\n",
    "        markov_matrix = np.zeros((180//self.square_size,360//self.square_size,9))\n",
    "        print(\"Generating Markov Matrix\")\n",
    "        for j, sorts in enumerate(unique_sorts):\n",
    "            \n",
    "            loadBar.load_bar(len(unique_sorts),j)\n",
    "            sort_df = df[df[sorting_column] == sorts]\n",
    "\n",
    "            #Iterate through all columns for input features\n",
    "            for i in range(len(sort_df)-1):\n",
    "                entry1 = sort_df.iloc[i]\n",
    "                entry2 = sort_df.iloc[i+1]\n",
    "\n",
    "                lat1 = entry1[\"latitude\"]\n",
    "                lon1 = entry1[\"longitude\"]\n",
    "\n",
    "                lat2 = entry2[\"latitude\"]\n",
    "                lon2 = entry2[\"longitude\"]\n",
    "\n",
    "                lat_diff = (lat1-lat2)//self.square_size\n",
    "                lon_diff = (lon1-lon2)//self.square_size\n",
    "\n",
    "                inner_idx=self.markov_index_inner(lat_diff, lon_diff)\n",
    "\n",
    "                lat_idx=int(lat1//self.square_size)\n",
    "                lon_idx=int(lon1//self.square_size)\n",
    "            \n",
    "                markov_matrix[lat_idx][lon_idx][inner_idx]+=1\n",
    "            \n",
    "        self.normalized_markov_matrix  = markov_matrix / np.sum(markov_matrix, axis=-1, keepdims=True)\n",
    "\n",
    "    def add_as_columns(self, df):\n",
    "        # Define column names for the new data\n",
    "        column_names = self.direction_columns\n",
    "        \n",
    "        # Initialize a DataFrame to hold the results\n",
    "        results_df = pd.DataFrame(columns=column_names, index=df.index)\n",
    "        \n",
    "        print(\"Adding Markov Squares as columns\")\n",
    "        i=0\n",
    "        for idx, row in df.iterrows():\n",
    "            i+=1\n",
    "            loadBar.load_bar(len(df),i)\n",
    "            # Extract latitude and longitude values\n",
    "            latitude = row[\"latitude\"]\n",
    "            longitude = row[\"longitude\"]\n",
    "            \n",
    "            # Check for NaN values in latitude and longitude\n",
    "            if pd.notna(latitude) and pd.notna(longitude):\n",
    "                # Get the processed values (assumes `get_markov_square` returns a list of 9 values)\n",
    "                processed_values = self.get_markov_square(latitude, longitude)\n",
    "                # Set the result in the corresponding row of results_df\n",
    "                results_df.loc[idx] = processed_values\n",
    "            else:\n",
    "                # Set NaN for each new column if latitude or longitude is NaN\n",
    "                results_df.loc[idx] = [np.nan] * 9\n",
    "\n",
    "        # Concatenate results_df with the original df along the columns axis\n",
    "        df = pd.concat([df, results_df], axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_markov_square(self, lat, lon):\n",
    "        lat_idx = int(lat//self.square_size)\n",
    "        lon_idx = int(lon//self.square_size)\n",
    "        return self.normalized_markov_matrix[lat_idx][lon_idx]\n",
    "\n",
    "    def markov_index_inner(self, lat_diff, lon_diff):\n",
    "        lat_index = 0 if lat_diff <= -1 else (1 if lat_diff == 0 else 2)\n",
    "        lon_index = 0 if lon_diff <= -1 else (1 if lon_diff == 0 else 2)\n",
    "        index = lat_index * 3 + lon_index\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.read_csv(RESOURCE_FOLDER+\"/data_resampled_20min.csv\")\n",
    "\n",
    "\n",
    "# total_df = pd.read_csv(RESOURCE_FOLDER+\"/resampled_data_h.csv\")\n",
    "\n",
    "total_df['etaParsed'] = pd.to_datetime(total_df['etaParsed'])\n",
    "total_df[\"time\"] = pd.to_datetime(total_df['time'])\n",
    "\n",
    "start_date = pd.to_datetime('2024-01-01')\n",
    "\n",
    "total_df[\"etaParsed\"] = (total_df['etaParsed'] - start_date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov = MarkovSquares(2)\n",
    "markov.markovSquares(total_df, \"vesselId\")\n",
    "np.save(markov.normalized_markov_matrix, RESOURCE_FOLDER+\"/markov_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = markov.add_as_columns(total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time_diffs = total_df[\"time\"].diff()\n",
    "time_interval = time_diffs.dropna().iloc[0]\n",
    "time_interval = int(time_interval.total_seconds()/(60*20))\n",
    "\n",
    "\n",
    "\n",
    "total_df.set_index(\"time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.to_csv(RESOURCE_FOLDER+\"/data_resampled_20min_markov.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(total_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.read_csv(RESOURCE_FOLDER+\"/data_resampled_20min_markov.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  time    cog  sog  rot  heading  navstat   latitude  \\\n",
      "0  2024-01-01 00:00:00  284.0  0.7  0.0     88.0      0.0 -34.743700   \n",
      "1  2024-01-01 00:20:00  284.0  0.7  0.0     88.0      0.0 -34.627229   \n",
      "2  2024-01-01 00:40:00  284.0  0.7  0.0     88.0      0.0 -34.510757   \n",
      "3  2024-01-01 01:00:00  284.0  0.7  0.0     88.0      0.0 -34.394286   \n",
      "4  2024-01-01 01:20:00  284.0  0.7  0.0     88.0      0.0 -34.277814   \n",
      "\n",
      "   longitude                  vesselId                    portId  ...  \\\n",
      "0 -57.851300  61e9f3a8b937134a3c4bfdf7  61d371c43aeaecc07011a37f  ...   \n",
      "1 -57.966135  61e9f3a8b937134a3c4bfdf7  61d371c43aeaecc07011a37f  ...   \n",
      "2 -58.080969  61e9f3a8b937134a3c4bfdf7  61d371c43aeaecc07011a37f  ...   \n",
      "3 -58.195804  61e9f3a8b937134a3c4bfdf7  61d371c43aeaecc07011a37f  ...   \n",
      "4 -58.310639  61e9f3a8b937134a3c4bfdf7  61d371c43aeaecc07011a37f  ...   \n",
      "\n",
      "   portLatitude        SW         S   SE         W         C    E   NW    N  \\\n",
      "0      -33.5875  0.078325  0.485714  0.0  0.348030  0.087931  0.0  0.0  0.0   \n",
      "1      -33.5875  0.078325  0.485714  0.0  0.348030  0.087931  0.0  0.0  0.0   \n",
      "2      -33.5875  0.089428  0.445416  0.0  0.374006  0.091150  0.0  0.0  0.0   \n",
      "3      -33.5875  0.089428  0.445416  0.0  0.374006  0.091150  0.0  0.0  0.0   \n",
      "4      -33.5875  0.089428  0.445416  0.0  0.374006  0.091150  0.0  0.0  0.0   \n",
      "\n",
      "    NE  \n",
      "0  0.0  \n",
      "1  0.0  \n",
      "2  0.0  \n",
      "3  0.0  \n",
      "4  0.0  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(total_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPSIZES = [1] #3, 6, 18, 36, 72, 144, 216, 288, 360]\n",
    "OUTPUT_WINDOW = 1\n",
    "INPUT_WINDOW = 4\n",
    "OUTPUT_FORECAST = [\"latitude\", \"longitude\", \"cog\", \"sog\", \"rot\", \"heading\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating supervised data\n",
      "[====================] 100.0% complete\n"
     ]
    }
   ],
   "source": [
    "#Make time series into supervised problem\n",
    "\n",
    "# 1 = 20 minutes\n",
    "# 3 = 1 hour\n",
    "# 18 = 6 hours\n",
    "# 72 = 24 hours\n",
    "# 144 = 2 days\n",
    "# 216 = 3 days\n",
    "# 288 = 4 days\n",
    "# 360 = 5 days\n",
    "\n",
    "\n",
    "\n",
    "def make_supervised(df, forecast_columns, sorting_column, input_window=1, output_window=1):\n",
    "    \"\"\"\n",
    "    Converts a multivariate time series dataframe into a supervised learning problem.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The original dataframe with time series data.\n",
    "    forecast_columns (list): A list of column names to forecast.\n",
    "    input_window (int): The number of past observations to use as features.\n",
    "    output_window (int): The number of steps to forecast into the future.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A new dataframe with supervised learning format.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    df_new = pd.DataFrame()\n",
    "    #Put in a for loop here where you iterate over all IDs, to make sure things get correct\n",
    "    unique_sorts = df[sorting_column].unique()\n",
    "\n",
    "    forbidden_cols = [\"vesselId\", \"UN_LOCODE\", \"ISO\", \"portId\", \"etaParsed\"]\n",
    "    \n",
    "    #Iterate through all IDs\n",
    "    print(\"Creating supervised data\")\n",
    "    for i, sorts in enumerate(unique_sorts):\n",
    "        \n",
    "        loadBar.load_bar(len(unique_sorts),i+1)\n",
    "        df_supervised = pd.DataFrame()\n",
    "        sort_df = df[df[sorting_column] == sorts]\n",
    "\n",
    "        #Iterate through all columns for input features\n",
    "        for col in sort_df.columns: \n",
    "            if col in forbidden_cols:\n",
    "                    continue\n",
    "            for i in range(input_window, 0, -1):\n",
    "                df_supervised[f\"{col}_t-{i}\"] = sort_df[col].shift(i)\n",
    "            \n",
    "            df_supervised[f\"{col}_t\"] = sort_df[col]\n",
    "            \n",
    "\n",
    "        # Create columns for forecast (target) with forward shift\n",
    "        for col in forecast_columns:\n",
    "            for j in range(output_window, 0,-1):\n",
    "                df_supervised[f\"{col}_t+{j}\"] = sort_df[col].shift(-j)\n",
    "        \n",
    "        df_new = pd.concat([df_new, df_supervised])\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# total_df = pd.read_csv(\"../../build_resources/data_resampled_20min_markov.csv\")\n",
    "\n",
    "total_df = make_supervised(total_df, OUTPUT_FORECAST, \"vesselId\", input_window=INPUT_WINDOW, output_window=OUTPUT_WINDOW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              time_t-4             time_t-3             time_t-2  \\\n",
      "0                 None                 None                 None   \n",
      "1                 None                 None                 None   \n",
      "2                 None                 None  2024-01-01 00:00:00   \n",
      "3                 None  2024-01-01 00:00:00  2024-01-01 00:20:00   \n",
      "4  2024-01-01 00:00:00  2024-01-01 00:20:00  2024-01-01 00:40:00   \n",
      "\n",
      "              time_t-1               time_t  cog_t-4  cog_t-3  cog_t-2  \\\n",
      "0                 None  2024-01-01 00:00:00      NaN      NaN      NaN   \n",
      "1  2024-01-01 00:00:00  2024-01-01 00:20:00      NaN      NaN      NaN   \n",
      "2  2024-01-01 00:20:00  2024-01-01 00:40:00      NaN      NaN    284.0   \n",
      "3  2024-01-01 00:40:00  2024-01-01 01:00:00      NaN    284.0    284.0   \n",
      "4  2024-01-01 01:00:00  2024-01-01 01:20:00    284.0    284.0    284.0   \n",
      "\n",
      "   cog_t-1  cog_t  ...  NE_t-3  NE_t-2  NE_t-1  NE_t  latitude_t+1  \\\n",
      "0      NaN  284.0  ...     NaN     NaN     NaN   0.0    -34.627229   \n",
      "1    284.0  284.0  ...     NaN     NaN     0.0   0.0    -34.510757   \n",
      "2    284.0  284.0  ...     NaN     0.0     0.0   0.0    -34.394286   \n",
      "3    284.0  284.0  ...     0.0     0.0     0.0   0.0    -34.277814   \n",
      "4    284.0  284.0  ...     0.0     0.0     0.0   0.0    -34.239269   \n",
      "\n",
      "   longitude_t+1  cog_t+1  sog_t+1  rot_t+1  heading_t+1  \n",
      "0     -57.966135    284.0      0.7      0.0         88.0  \n",
      "1     -58.080969    284.0      0.7      0.0         88.0  \n",
      "2     -58.195804    284.0      0.7      0.0         88.0  \n",
      "3     -58.310639    284.0      0.7      0.0         88.0  \n",
      "4     -58.315756    284.0      0.7      0.0         88.0  \n",
      "\n",
      "[5 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the first few rows of the dataframe\n",
    "total_df.to_csv(RESOURCE_FOLDER+\"/data_resampled_20min_markov_supervised.csv\")\n",
    "print(total_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         latitude_t+1  longitude_t+1  cog_t+1  sog_t+1  rot_t+1  heading_t+1\n",
      "0          -34.627229     -57.966135    284.0      0.7      0.0         88.0\n",
      "1          -34.510757     -58.080969    284.0      0.7      0.0         88.0\n",
      "2          -34.394286     -58.195804    284.0      0.7      0.0         88.0\n",
      "3          -34.277814     -58.310639    284.0      0.7      0.0         88.0\n",
      "4          -34.239269     -58.315756    284.0      0.7      0.0         88.0\n",
      "...               ...            ...      ...      ...      ...          ...\n",
      "5899716     37.110410     144.761650     58.2     15.7      0.0         51.0\n",
      "5899717     37.157450     144.851640     57.7     15.4      0.0         50.0\n",
      "5899718     37.204090     144.947320     58.7     15.1      0.0         48.0\n",
      "5899719     37.222310     144.984520     59.8     14.9      0.0         48.0\n",
      "5899720           NaN            NaN      NaN      NaN      NaN          NaN\n",
      "\n",
      "[5899721 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Sorting columns\n",
    "def sort_columns(df):\n",
    "    \n",
    "    # Extract suffixes and assign _t as _t0\n",
    "    columns_with_suffix = []\n",
    "    for col in df.columns:\n",
    "        match = re.search(r\"_t([+-]?\\d*)$\", col)\n",
    "        # If there's no number after _t, treat it as _t0\n",
    "        suffix = int(match.group(1)) if match.group(1) else 0\n",
    "        columns_with_suffix.append((col, suffix))\n",
    "    \n",
    "    # Sort by suffix value (ascending)\n",
    "    sorted_t_columns = [col for col, _ in sorted(columns_with_suffix, key=lambda x: x[1])]\n",
    "    \n",
    "    # Reorder dataframe columns\n",
    "    return df[sorted_t_columns]\n",
    "\n",
    "\n",
    "total_df = total_df.sort_index(ascending = True)\n",
    "total_df=sort_columns(total_df)\n",
    "\n",
    "print(total_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_test_split(df, perc1, perc2, output_window):\n",
    "    y_list = []\n",
    "    for j in range(output_window):\n",
    "        for col in OUTPUT_FORECAST:\n",
    "            y_list.append(f\"{col}_t+{j+1}\")\n",
    "    ys = df[y_list]\n",
    "    Xs = df.drop(columns = y_list)\n",
    "\n",
    "    X_train = Xs.iloc[:int(np.round(Xs.shape[0]*perc1)),:]\n",
    "    y_train = ys.iloc[:int(np.round(Xs.shape[0]*perc1)),:]\n",
    "    X_val = Xs.iloc[int(np.round(Xs.shape[0]*perc1)):int(np.round(Xs.shape[0]*perc2)),:]\n",
    "    y_val = ys.iloc[int(np.round(Xs.shape[0]*perc1)):int(np.round(Xs.shape[0]*perc2)),:]\n",
    "    X_test = Xs.iloc[int(np.round(Xs.shape[0]*perc2)):,:]\n",
    "    y_test = ys.iloc[int(np.round(Xs.shape[0]*perc2)):,:]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_test_split(total_df, 0.75, 0.85, OUTPUT_WINDOW)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(stepsize, preds, y_val):\n",
    "    print(\"/\"+\"-\"*50+\"\\\\\")\n",
    "    print(\"Evaluating model with stepsize\", stepsize)\n",
    "\n",
    "    results = {\n",
    "        \"MAE\": mean_absolute_error(y_val, preds),\n",
    "        \"MSE\": np.square(np.subtract(y_val,preds)).mean(),\n",
    "        \"R2 Score\": r2_score(y_val, preds),\n",
    "        \"RMSE\": np.sqrt(np.square(np.subtract(y_val,preds)).mean())\n",
    "    }\n",
    "\n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(\"\\\\\"+\"-\"*50+\"/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
      "\n",
      "[4424791 rows x 0 columns]\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[15:39:04] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\data\\data.cc:514: Check failed: valid: Label contains NaN, infinity or a value too large.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train)\n\u001b[1;32m----> 5\u001b[0m dtrain \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m dval \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X_val, label\u001b[38;5;241m=\u001b[39my_val)\n\u001b[0;32m      7\u001b[0m dtest_X \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:890\u001b[0m, in \u001b[0;36mDMatrix.__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m handle\n\u001b[1;32m--> 890\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names \u001b[38;5;241m=\u001b[39m feature_names\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:954\u001b[0m, in \u001b[0;36mDMatrix.set_info\u001b[1;34m(self, label, weight, base_margin, group, qid, label_lower_bound, label_upper_bound, feature_names, feature_types, feature_weights)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_meta_backend\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 954\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_weight(weight)\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:1092\u001b[0m, in \u001b[0;36mDMatrix.set_label\u001b[1;34m(self, label)\u001b[0m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Set label of dmatrix\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m \n\u001b[0;32m   1085\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;124;03m    The label information to be set into DMatrix\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_meta_backend\n\u001b[1;32m-> 1092\u001b[0m \u001b[43mdispatch_meta_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\data.py:1359\u001b[0m, in \u001b[0;36mdispatch_meta_backend\u001b[1;34m(matrix, data, name, dtype)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     data \u001b[38;5;241m=\u001b[39m _arrow_transform(data)\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df(data):\n\u001b[1;32m-> 1359\u001b[0m     \u001b[43m_meta_from_pandas_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_series(data):\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\data.py:628\u001b[0m, in \u001b[0;36m_meta_from_pandas_df\u001b[1;34m(data, name, dtype, handle)\u001b[0m\n\u001b[0;32m    625\u001b[0m     array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(data\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    627\u001b[0m array, dtype \u001b[38;5;241m=\u001b[39m _ensure_np_dtype(array, dtype)\n\u001b[1;32m--> 628\u001b[0m \u001b[43m_meta_from_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\data.py:1295\u001b[0m, in \u001b[0;36m_meta_from_numpy\u001b[1;34m(data, field, dtype, handle)\u001b[0m\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMasked array is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1294\u001b[0m interface_str \u001b[38;5;241m=\u001b[39m _array_interface(data)\n\u001b[1;32m-> 1295\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGDMatrixSetInfoFromInterface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:284\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [15:39:04] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\data\\data.cc:514: Check failed: valid: Label contains NaN, infinity or a value too large."
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_train)\n",
    "\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "dtest_X = xgb.DMatrix(X_test)\n",
    "\n",
    "params = {\"objective\": \"reg:squarederror\",\n",
    "            \"max_depth\": 5,\n",
    "            \"booster\": \"gbtree\",\n",
    "            \"tree-method\": \"gpu_hist\",\n",
    "            \"col_sample_bynode\": 0.5,\n",
    "            \"num_parallel_tree\": 100,\n",
    "            \"subsample\": 0.8,\n",
    "            \"learning_rate\": 1,\n",
    "            #\"n_estimators\": 100,\n",
    "            #\"reg_alpha\": 0.1,\n",
    "            #\"reg_lambda\": 0.1,\n",
    "            #\"n_jobs\": -1,\n",
    "            \"verbosity\": 1\n",
    "            }\n",
    "\n",
    "num_boost_round = 5\n",
    "\n",
    "early_stopping_rounds = 2\n",
    "\n",
    "print(dtrain)\n",
    "\n",
    "model = xgb.train(params, dtrain, num_boost_round, evals=[(dval, \"validation\")], early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
    "\n",
    "\n",
    "preds = model.predict(dtest_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_n_min_mark(timestamp, n=1):\n",
    "    timestamp = pd.to_datetime(timestamp)\n",
    "    minutes = timestamp.minute\n",
    "    closest_mark = round(minutes / n*20) * n*20\n",
    "    if closest_mark == 60:\n",
    "        rounded_timestamp = timestamp.replace(minute=0, second=0, microsecond=0) + pd.Timedelta(hours=1)\n",
    "    else:\n",
    "        rounded_timestamp = timestamp.replace(minute=closest_mark, second=0, microsecond=0)\n",
    "    \n",
    "    return rounded_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_to_back(process_df):      \n",
    "    for _, col in enumerate(OUTPUT_FORECAST):\n",
    "\n",
    "        max_suffix_neg = 0\n",
    "        max_suffix_pos = 0\n",
    "        \n",
    "        # Identify existing suffixes in the process_df for the current column\n",
    "        while f\"{col}_t-{max_suffix_neg+1}\" in process_df.columns:\n",
    "            max_suffix_neg += 1\n",
    "        while f\"{col}_t+{max_suffix_pos+1}\" in process_df.columns:\n",
    "            max_suffix_pos += 1\n",
    "        for shift in range(max_suffix_neg - 1, -max_suffix_pos, -1):  # Start from max_suffix-1 down to 0\n",
    "            if shift == 0:\n",
    "                # Set the new predicted value as the most recent\n",
    "                process_df[f\"{col}_t\"] = process_df[f\"{col}_t+1\"]\n",
    "            elif shift == 1:\n",
    "                # Shift the column\n",
    "                process_df[f\"{col}_t-{shift}\"] = process_df[f\"{col}_t\"]\n",
    "            elif shift > 1:\n",
    "                # Shift the column\n",
    "                process_df[f\"{col}_t-{shift}\"] = process_df[f\"{col}_t-{shift - 1}\"]\n",
    "            else:\n",
    "                process_df[f\"{col}_t+{-shift}\"] = process_df[f\"{col}_t+{-shift + 1}\"]\n",
    "\n",
    "        for shift in range(1, max_suffix_pos+1):\n",
    "            process_df = process_df.drop(columns=[f\"{col}_t+{shift}\"])\n",
    "    \n",
    "    return process_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_far_future(model, features, test_df,  forecast_columns):\n",
    "    \n",
    "    X_test = features.copy().iloc[-1:]\n",
    "    preds = pd.DataFrame(columns=[\"vesselId\", \"approximate_time\"])\n",
    "    \n",
    "    # Determine the furthest time in 20-minute intervals\n",
    "    furthest_time = closest_n_min_mark(test_df[\"time\"].max())\n",
    "    current_time = closest_n_min_mark(X_test.index.max())\n",
    "    \n",
    "    # Generate the future time steps at 20-minute intervals\n",
    "    future_steps = pd.date_range(start=current_time, end=furthest_time, freq='20min')\n",
    "    \n",
    "    for future_time in future_steps:\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        new_row = pd.DataFrame({\n",
    "            \"vesselId\": [test_df[\"vesselId\"].iloc[0]],\n",
    "            \"approximate_time\": [future_time]\n",
    "        })\n",
    "        for idx, col in enumerate(forecast_columns):\n",
    "            new_row[f\"{col}\"] = y_pred[0, idx]  # Use the predicted value\n",
    "        new_row = markov.add_as_columns(new_row)\n",
    "        \n",
    "        \n",
    "        preds = pd.concat([preds, new_row], ignore_index=True)\n",
    "        \n",
    "        # Update X_test for the next iteration\n",
    "        for idx, col in enumerate(forecast_columns):\n",
    "\n",
    "            max_suffix = 0\n",
    "            \n",
    "            # Identify existing suffixes in the X_test for the current column\n",
    "            while f\"{col}_t-{max_suffix+1}\" in X_test.columns:\n",
    "                max_suffix += 1\n",
    "            for shift in range(max_suffix - 1, -1, -1):  # Start from max_suffix-1 down to 0\n",
    "                if shift == 0:\n",
    "                    # Set the new predicted value as the most recent\n",
    "                    X_test[f\"{col}_t\"] = y_pred[0, idx]\n",
    "                elif shift == 1:\n",
    "                    # Shift the column\n",
    "                    X_test[f\"{col}_t-{shift}\"] = X_test[f\"{col}_t\"]\n",
    "                else:\n",
    "                    # Shift the column\n",
    "                    X_test[f\"{col}_t-{shift}\"] = X_test[f\"{col}_t-{shift - 1}\"]\n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n",
    "csv_parser = CSVParser(folderpath=RESOURCE_FOLDER)\n",
    "\n",
    "test_df = csv_parser.retrieve_test_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_times(model,total_df,test_df):\n",
    "    unique_sorts = test_df[\"vesselId\"].unique()\n",
    "    preds_df = pd.DataFrame()\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    for sorts in unique_sorts:\n",
    "        latest_features=total_df[total_df[\"vesselId\"] == sorts]\n",
    "        test_by_vessel_df = test_df[test_df[\"vesselId\"] == sorts]\n",
    "        latest_features = make_supervised(latest_features, OUTPUT_FORECAST, \"vesselId\" , INPUT_WINDOW, OUTPUT_WINDOW)\n",
    "        latest_features = shift_to_back(latest_features)\n",
    "        latest_features = sort_columns(latest_features)\n",
    "        preds = predict_far_future(model, latest_features, test_by_vessel_df, OUTPUT_FORECAST)\n",
    "        preds_df = pd.concat([preds_df, preds])\n",
    "    \n",
    "    for test in test_df.iterrows():\n",
    "        test=pd.Series(test[1])\n",
    "        new_row = pd.DataFrame()\n",
    "        new_row=preds_df[\n",
    "            (preds_df[\"vesselId\"] == test[\"vesselId\"]) & \n",
    "            (preds_df[\"approximate_time\"] == closest_n_min_mark(test[\"time\"]))\n",
    "            ][[\"latitude\", \"longitude\"]]\n",
    "        new_row[\"ID\"] = test[\"ID\"]\n",
    "        new_row[\"time\"] = test[\"time\"]\n",
    "        \n",
    "        result = pd.concat([result, new_row])\n",
    "    result[\"latitude_predicted\"] = result[\"latitude\"]\n",
    "    result[\"longitude_predicted\"] = result[\"longitude\"]\n",
    "\n",
    "    return result[[\"ID\",\"longitude_predicted\",\"latitude_predicted\"]]\n",
    "\n",
    "print(test_df)\n",
    "result_df = predict_times(model, total_df, test_df)\n",
    "print(result_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn results into a csv file\n",
    "result_df.to_csv(RESOURCE_FOLDER+\"/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_importance(model, height=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model:\n",
    "Included navstat and etaParsed\n",
    "\n",
    "Timewindow: (3,2)\n",
    "\n",
    "MAE: 0.8521843262281953 \n",
    "\n",
    "MSE: longitude_t+1    21.225563\n",
    "\n",
    "latitude_t+1      1.993130\n",
    "\n",
    "longitude_t+2    38.471488\n",
    "\n",
    "latitude_t+2      3.840146\n",
    "\n",
    "dtype: float64\n",
    "\n",
    "R2 Score: 0.9958523607729776\n",
    "\n",
    "RMSE: longitude_t+1    4.607121\n",
    "\n",
    "latitude_t+1     1.411783\n",
    "\n",
    "longitude_t+2    6.202539\n",
    "\n",
    "latitude_t+2     1.959629\n",
    "\n",
    "dtype: float64\n",
    "\n",
    "\n",
    "### Second model:\n",
    "\n",
    "Added cog, rot and heading to target features.\n",
    "\n",
    "Timewindow: (3,2)\n",
    "\n",
    "MAE: 7.198335594601071\n",
    "MSE: latitude_t+1        1.980426\n",
    "longitude_t+1      21.577318\n",
    "cog_t+1          1820.208937\n",
    "rot_t+1            92.532501\n",
    "heading_t+1      1172.604934\n",
    "latitude_t+2        3.813640\n",
    "longitude_t+2      39.218475\n",
    "cog_t+2          2370.325440\n",
    "rot_t+2           107.991661\n",
    "heading_t+2      1769.347459\n",
    "dtype: float64\n",
    "R2 Score: 0.8826565909012996\n",
    "RMSE: latitude_t+1      1.407276\n",
    "longitude_t+1     4.645139\n",
    "cog_t+1          42.663907\n",
    "rot_t+1           9.619382\n",
    "heading_t+1      34.243320\n",
    "latitude_t+2      1.952854\n",
    "longitude_t+2     6.262466\n",
    "cog_t+2          48.685988\n",
    "rot_t+2          10.391904\n",
    "heading_t+2      42.063612\n",
    "dtype: float64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
